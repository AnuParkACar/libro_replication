{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Day 2 Dry Run on GPU\n",
    "Generate tests for 5 bugs using A100 GPU\n",
    "\n",
    "**IMPORTANT: Go to Runtime → Change runtime type → Select A100 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️  No GPU detected! Change runtime type to A100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract-code"
   },
   "outputs": [],
   "source": [
    "# Extract code\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "work_dir = Path('/content/libro_replication')\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# Unzip code from Drive\n",
    "!unzip -q /content/drive/MyDrive/libro_replication/libro_code.zip -d /content/libro_replication\n",
    "\n",
    "print(\"✓ Code extracted\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch accelerate huggingface_hub sentencepiece -q\n",
    "!pip install tqdm pyyaml -q\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf-login"
   },
   "outputs": [],
   "source": [
    "# HuggingFace login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('/content/libro_replication')\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from src.model_manager import ModelManager\n",
    "from src.core.prompt_builder import PromptBuilder\n",
    "from src.core.test_generator import TestGenerator\n",
    "from src.core.batch_processor import BatchProcessor\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-bugs"
   },
   "outputs": [],
   "source": [
    "# Load bug reports\n",
    "with open('data/bugs/bug_reports.json') as f:\n",
    "    all_bug_reports = json.load(f)\n",
    "\n",
    "# Select first 5 bugs for dry run\n",
    "bug_reports = all_bug_reports[:5]\n",
    "\n",
    "print(f\"Loaded {len(bug_reports)} bugs for dry run:\")\n",
    "for bug in bug_reports:\n",
    "    print(f\"  - {bug['project']}-{bug['bug_id']}: {bug['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "# Initialize Model (THIS WILL TAKE 5-10 MINUTES)\n",
    "print(\"Loading model... (this will take 5-10 minutes)\")\n",
    "\n",
    "model_manager = ModelManager(\n",
    "    model_key=\"starcoder2-15b\",\n",
    "    cache_dir=\"/content/drive/MyDrive/libro_replication/models/starcoder2-15b\"\n",
    ")\n",
    "\n",
    "model_manager.load()\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "print(model_manager.get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init-prompt"
   },
   "outputs": [],
   "source": [
    "# Initialize prompt builder\n",
    "prompt_builder = PromptBuilder(\n",
    "    num_examples=2,\n",
    "    examples_file=\"data/examples/manual_examples.json\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Prompt builder initialized with {len(prompt_builder.examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init-generator"
   },
   "outputs": [],
   "source": [
    "# Initialize test generator\n",
    "test_generator = TestGenerator(\n",
    "    model_manager=model_manager,\n",
    "    cache_dir=\"/content/drive/MyDrive/libro_replication/cache/generations\"\n",
    ")\n",
    "\n",
    "print(\"✓ Test generator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init-batch"
   },
   "outputs": [],
   "source": [
    "# Initialize batch processor\n",
    "batch_processor = BatchProcessor(\n",
    "    prompt_builder=prompt_builder,\n",
    "    test_generator=test_generator,\n",
    "    output_dir=\"/content/drive/MyDrive/libro_replication/results/dry_run_gpu\"\n",
    ")\n",
    "\n",
    "print(\"✓ Batch processor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-generation"
   },
   "outputs": [],
   "source": [
    "# Run generation (10-15 minutes)\n",
    "print(\"Starting generation...\")\n",
    "print(\"This will take approximately 10-15 minutes\")\n",
    "print()\n",
    "\n",
    "results = batch_processor.process_bugs(\n",
    "    bug_reports=bug_reports,\n",
    "    n_samples=10\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary"
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_bugs = len(results)\n",
    "total_tests = sum(len(tests) for tests in results.values())\n",
    "bugs_with_tests = sum(1 for tests in results.values() if tests)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DRY RUN RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal bugs processed: {total_bugs}\")\n",
    "print(f\"Bugs with ≥1 test: {bugs_with_tests} ({bugs_with_tests/total_bugs*100:.1f}%)\")\n",
    "print(f\"Total tests generated: {total_tests}\")\n",
    "print(f\"Average tests per bug: {total_tests/total_bugs:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-samples"
   },
   "outputs": [],
   "source": [
    "# Show sample tests\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for bug_id, tests in results.items():\n",
    "    print(f\"\\n{bug_id}: {len(tests)} tests generated\")\n",
    "    \n",
    "    if tests:\n",
    "        print(\"  First test:\")\n",
    "        print(\"  \" + \"-\" * 70)\n",
    "        for line in tests[0]['test_code'].split('\\n')[:15]:\n",
    "            print(f\"  {line}\")\n",
    "        if len(tests[0]['test_code'].split('\\n')) > 15:\n",
    "            print(\"  ...\")\n",
    "        print(f\"\\n  Generation time: {tests[0].get('generation_time', 0):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality"
   },
   "outputs": [],
   "source": [
    "# Quality analysis\n",
    "import re\n",
    "\n",
    "def check_quality(test_code):\n",
    "    return {\n",
    "        'has_signature': bool(re.search(r'public\\s+void\\s+test', test_code)),\n",
    "        'has_assertion': bool(re.search(r'assert\\w+\\(', test_code, re.IGNORECASE)),\n",
    "        'balanced_braces': test_code.count('{') == test_code.count('}'),\n",
    "        'has_content': len(test_code.strip()) > 50\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_tests = [test for tests in results.values() for test in tests]\n",
    "quality_scores = [sum(check_quality(t['test_code']).values()) for t in all_tests]\n",
    "\n",
    "print(f\"\\nQuality distribution (out of 4 checks):\")\n",
    "print(f\"  Perfect (4/4): {sum(1 for s in quality_scores if s == 4)} tests\")\n",
    "print(f\"  Good (3/4): {sum(1 for s in quality_scores if s == 3)} tests\")\n",
    "print(f\"  Acceptable (2/4): {sum(1 for s in quality_scores if s == 2)} tests\")\n",
    "print(f\"  Poor (<2/4): {sum(1 for s in quality_scores if s < 2)} tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-saved"
   },
   "outputs": [],
   "source": [
    "# Verify files saved to Drive\n",
    "output_dir = Path(\"/content/drive/MyDrive/libro_replication/results/dry_run_gpu\")\n",
    "\n",
    "print(\"Files saved to Google Drive:\")\n",
    "for file in output_dir.glob(\"*\"):\n",
    "    print(f\"  ✓ {file.name}\")\n",
    "\n",
    "print(\"\\nYou can find these files in:\")\n",
    "print(\"  Google Drive → libro_replication → results → dry_run_gpu/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip-results"
   },
   "outputs": [],
   "source": [
    "# Create zip for easy download\n",
    "!cd /content/drive/MyDrive/libro_replication/results && zip -r dry_run_results.zip dry_run_gpu/\n",
    "\n",
    "print(\"✓ Results zipped\")\n",
    "print(\"Download from: Google Drive → libro_replication → results → dry_run_results.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-md"
   },
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Completed:**\n",
    "- Generated tests for 5 bugs\n",
    "- 10 samples per bug\n",
    "- Used StarCoder2-15B on A100 GPU\n",
    "- Results saved to Google Drive\n",
    "\n",
    "**Files in Google Drive:**\n",
    "- `generation_results.json` - Raw generation data\n",
    "- `generation_stats.json` - Summary statistics\n",
    "- `progress.json` - Progress tracking\n",
    "- Individual bug caches in `cache/generations/`\n",
    "\n",
    "**Next Steps:**\n",
    "- Day 3: Implement test injection and execution\n",
    "- Download results and continue locally or expand to 30 bugs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
